{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "124f6c1f",
   "metadata": {},
   "source": [
    "## Zarr DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005368bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import natsort\n",
    "import xarray as xr\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "sys.path.append('../')\n",
    "from data.healpix import *\n",
    "from utils.plot import plot_all_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa51adcb",
   "metadata": {},
   "source": [
    "The main goal of this Notebook is to create a Datasets and DataLoader that manipulate Zarr and its associated chunks. PyTorch provides two data primitives: torch.utils.data.DataLoader and torch.utils.data.Dataset that allow you to use pre-loaded datasets as well as your own data. Dataset stores the samples and their corresponding labels, and DataLoader wraps an iterable around the Dataset to enable easy access to the samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a959979",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a07345",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_paths(path_dir):\n",
    "\n",
    "    df_input = pd.read_csv(f\"{path_dir}/input.csv\")\n",
    "    df_output = pd.read_csv(f\"{path_dir}/target.csv\")\n",
    "    df_input[\"path\"] = df_input[\"Name\"].apply(lambda x: os.path.join(path_dir, \"input\", os.path.basename(x).replace(\".SAFE\",\"\")))\n",
    "    df_output[\"path\"] = df_output[\"Name\"].apply(lambda x: os.path.join(path_dir, \"target\", os.path.basename(x).replace(\".SAFE\",\"\")))\n",
    "\n",
    "    return df_input, df_output\n",
    "\n",
    "base_dir = \"/mnt/disk/dataset/sentinel-ai-processor\"\n",
    "version = \"V4\"\n",
    "\n",
    "TRAIN_DIR = f\"{base_dir}/{version}/train/\"\n",
    "VAL_DIR = f\"{base_dir}/{version}/val/\"\n",
    "TEST_DIR = f\"{base_dir}/{version}/test/\"\n",
    "df_train_input, df_train_output =  prepare_paths(TRAIN_DIR)\n",
    "df_val_input, df_val_output =  prepare_paths(VAL_DIR)\n",
    "df_test_input, df_test_output =  prepare_paths(TEST_DIR)\n",
    "df_test_output = df_test_output[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4b9a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "zarr_index = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02809c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open .zarr datatree\n",
    "x_path = df_train_output[\"path\"].iloc[zarr_index] + \".zarr\"\n",
    "dt = xr.open_datatree(x_path, engine=\"zarr\", chunks={})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2324c952",
   "metadata": {},
   "source": [
    "## Plot all chunks for a given resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f93076",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = \"60m\"\n",
    "bands = get_bands(data_tree=dt, res=res)\n",
    "bands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669f114a",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size_y, chunk_size_x, nb_chunks_y, nb_chunks_x = get_chunk_info(data_tree=dt, band=bands[0], res=res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554cea6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for band in bands:\n",
    "\n",
    "#     plot_all_chunks(dt  , band, res, chunk_size_y, chunk_size_x, nb_chunks_y, nb_chunks_x, save=True, filename=band ,  cmap=\"viridis\", verbose= False, figsize_scale=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4206162d",
   "metadata": {},
   "source": [
    "## Extract Chunk from Xarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419dbf2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_y_idx = 4\n",
    "chunk_x_idx = 4\n",
    "chunk = get_chunk(data_tree=dt, res=res,\n",
    "                  chunk_y_idx=chunk_y_idx, chunk_x_idx=chunk_x_idx,\n",
    "                  chunk_size_y=chunk_size_y, chunk_size_x=chunk_size_x)\n",
    "\n",
    "chunk_array = chunk.to_dataset().to_dataarray()\n",
    "chunk_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de9fea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_array.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b97f9c",
   "metadata": {},
   "source": [
    "## Datasets & DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff28352",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Sentinel2ZarrDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Loads Sentinel-2 data stored in Zarr format and returns all chunks for a scene.\n",
    "\n",
    "    Returns:\n",
    "        chunks_grid: Tensor of shape [nb_chunks_y, nb_chunks_x, bands, H, W]\n",
    "        meta: Tuple (nb_chunks_y, nb_chunks_x, chunk_size_y, chunk_size_x)\n",
    "    \"\"\"\n",
    "    def __init__(self, df_x, res, bands):\n",
    "        self.df_x = df_x\n",
    "        self.res = res\n",
    "        self.bands = bands\n",
    "\n",
    "        self.res_key = f\"r{res}\"\n",
    "        self.x_res = f\"x_{res}\"\n",
    "        self.y_res = f\"y_{res}\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df_x)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # --- Open Zarr scene ---\n",
    "        zarr_path = self.df_x[\"path\"].iloc[index] + \".zarr\"\n",
    "        datatree = xr.open_datatree(zarr_path, engine=\"zarr\", mask_and_scale=False, chunks={})\n",
    "        data = datatree.measurements.reflectance[self.res_key]\n",
    "\n",
    "        # --- Get chunk layout ---\n",
    "        band  = self.bands[0]\n",
    "        chunk_size_y = data[band].chunksizes[self.y_res][0]\n",
    "        chunk_size_x = data[band].chunksizes[self.x_res][0]\n",
    "        nb_chunks_y = len(data[band].chunksizes[self.y_res])\n",
    "        nb_chunks_x = len(data[band].chunksizes[self.x_res])\n",
    "\n",
    "        # --- Collect all chunks ---\n",
    "        all_chunks = []\n",
    "        for row in range(nb_chunks_y):  # Y direction\n",
    "            for col in range(nb_chunks_x):  # X direction\n",
    "                y_start = row * chunk_size_y\n",
    "                x_start = col * chunk_size_x\n",
    "\n",
    "                # Select chunk from all bands\n",
    "                chunk_ds = data.isel(\n",
    "                    {self.y_res: slice(y_start, y_start + chunk_size_y),\n",
    "                     self.x_res: slice(x_start, x_start + chunk_size_x)}\n",
    "                )\n",
    "\n",
    "                # Convert to tensor: [bands, H, W]\n",
    "                chunk_array = chunk_ds.to_dataset().to_dataarray().values\n",
    "                chunk_tensor = torch.from_numpy(chunk_array)\n",
    "                all_chunks.append(chunk_tensor)\n",
    "\n",
    "        # --- Stack into [nb_chunks_y, nb_chunks_x, bands, H, W] ---\n",
    "        all_chunks = torch.stack(all_chunks)\n",
    "        chunks_grid = all_chunks.view(nb_chunks_y, nb_chunks_x, *all_chunks.shape[1:])\n",
    "\n",
    "        meta = (nb_chunks_y, nb_chunks_x, chunk_size_y, chunk_size_x)\n",
    "        return chunks_grid, meta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e29524",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45be903a",
   "metadata": {},
   "source": [
    "Total batch number is equal to Number of zarr * nb_chunks_x * nb_chunks_y\n",
    "\n",
    "example: \n",
    " - 60m resolution\n",
    " - nb_chunks_x = 6 \n",
    " - nb_chunks_y = 6 \n",
    " - batch_size in train_loader = 2\n",
    "\n",
    "final batch = batch_size* nb_chunks_x * nb_chunks_y * = 72\n",
    "\n",
    "Final ouput ->>>>  [72, 11, 305, 305]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f0b311",
   "metadata": {},
   "source": [
    "## Check data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f219f2f",
   "metadata": {},
   "source": [
    "Let's take a random index in the entire dataset. batch = len of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d70e4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "# --------- Parameters ----------\n",
    "res = \"60m\"\n",
    "bands = ['b01', 'b02', 'b03', 'b04', 'b05', 'b06', 'b07', 'b09', 'b11', 'b12', 'b8a']\n",
    "batch_size = 2\n",
    "\n",
    "train_dataset = Sentinel2ZarrDataset(df_x=df_test_output, res=res, bands=bands)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "with tqdm(total=len(train_loader.dataset), ncols=100, colour='#3eedc4') as t:\n",
    "\n",
    "    t.set_description(\"Training\")\n",
    "    for chunks_grid, _ in train_loader:\n",
    "        # chunks_grid: [B, nb_chunks_y, nb_chunks_x, C, H, W]\n",
    "        # Flatten chunk grid â†’ [B * nb_chunks_y * nb_chunks_x, C, H, W]\n",
    "        B, ny, nx, C, H, W = chunks_grid.shape\n",
    "        chunks_tensor = chunks_grid.view(B * ny * nx, C, H, W).to(device)\n",
    "        print(chunks_tensor.shape)\n",
    "        t.update(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56da482b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Final output shape {chunks_tensor.shape} - [B, C, H, W]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941dc975",
   "metadata": {},
   "source": [
    "# Batch Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a076aac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b5eacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = \"60m\"\n",
    "bands = ['b01', 'b02', 'b03', 'b04', 'b05', 'b06', 'b07', 'b09', 'b11', 'b12', 'b8a']\n",
    "df_test_input, df_test_output =  prepare_paths(TEST_DIR)\n",
    "batch_size = len(df_test_output)\n",
    "train_dataset = Sentinel2ZarrDataset(df_x=df_test_output, res=res, bands=bands)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "with tqdm(total=len(train_loader.dataset), ncols=100, colour='#3eedc4') as t:\n",
    "    t.set_description(\"Training\")\n",
    "\n",
    "    for batch_idx, (chunks_grid, _) in enumerate(train_loader):\n",
    "        # chunks_grid: [B, nb_chunks_y, nb_chunks_x, C, H, W]\n",
    "        B, ny, nx, C, H, W = chunks_grid.shape\n",
    "        chunks_tensor = chunks_grid.view(B * ny * nx, C, H, W).to(device)\n",
    "\n",
    "        print(chunks_tensor.shape)\n",
    "\n",
    "        # Loop over each scene in the batch\n",
    "        for batch_scene in range(B):\n",
    "            scene_index = batch_idx * batch_size + batch_scene\n",
    "            scene_chunks = chunks_grid[batch_scene]  # [ny, nx, C, H, W]\n",
    "\n",
    "            # Pick a position in the chunk grid\n",
    "            row_idx = 0\n",
    "            col_idx = 4\n",
    "\n",
    "            # Get rebuilt chunk from dataset tensor\n",
    "            rebuilt_chunk = scene_chunks[row_idx, col_idx]  # [C, H, W]\n",
    "\n",
    "            # Load same chunk directly from Zarr\n",
    "            x_path = df_test_output[\"path\"].iloc[scene_index] + \".zarr\"\n",
    "            dt = xr.open_datatree(x_path, engine=\"zarr\", mask_and_scale=False, chunks={})\n",
    "            data_tree = dt.measurements.reflectance[f\"r{res}\"]\n",
    "\n",
    "            # Compute pixel indices in full image\n",
    "            chunk_size_y = H\n",
    "            chunk_size_x = W\n",
    "            y_start = row_idx * chunk_size_y\n",
    "            x_start = col_idx * chunk_size_x\n",
    "\n",
    "            original_chunk = data_tree.isel(\n",
    "                {f\"y_{res}\": slice(y_start, y_start + chunk_size_y),\n",
    "                 f\"x_{res}\": slice(x_start, x_start + chunk_size_x)}\n",
    "            ).to_dataset().to_dataarray()\n",
    "\n",
    "            # --- Plot rebuilt ---\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            plt.imshow(rebuilt_chunk[0].cpu().numpy(), cmap=\"viridis\")\n",
    "            plt.title(f\"Rebuilt - Scene {scene_index} - Chunk ({row_idx}, {col_idx}) - Band 0\")\n",
    "            plt.colorbar()\n",
    "            plt.show()\n",
    "\n",
    "            # --- Plot original ---\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            plt.imshow(original_chunk[0].values, cmap=\"viridis\")\n",
    "            plt.title(f\"Original - Scene {scene_index} - Chunk ({row_idx}, {col_idx}) - Band 0\")\n",
    "            plt.colorbar()\n",
    "            plt.show()\n",
    "\n",
    "        t.update(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046c2f7d",
   "metadata": {},
   "source": [
    "## With Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a87cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "\n",
    "# ---------------- Normalization ----------------\n",
    "def normalize(data_array):\n",
    "    normalized_data = []\n",
    "    valid_masks = []\n",
    "    for i in range(data_array.shape[2]):\n",
    "        band_data = data_array[:, :, i]\n",
    "        valid_mask = (band_data > 0)\n",
    "        result = band_data.copy().astype(np.float32)\n",
    "        result[~valid_mask] = 0.0\n",
    "        normalized_data.append(result)\n",
    "        valid_masks.append(valid_mask)\n",
    "    return np.dstack(normalized_data), np.dstack(valid_masks)\n",
    "\n",
    "# ---------------- Dataset ----------------\n",
    "class Sentinel2ZarrDataset(Dataset):\n",
    "    def __init__(self, df_x, res, bands, target_size=(320, 320)):\n",
    "        self.df_x = df_x\n",
    "        self.res = res\n",
    "        self.bands = bands\n",
    "        self.target_size = target_size\n",
    "        self.res_key = f\"r{res}\"\n",
    "        self.x_res = f\"x_{res}\"\n",
    "        self.y_res = f\"y_{res}\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df_x)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        zarr_path = self.df_x[\"path\"].iloc[index] + \".zarr\"\n",
    "        datatree = xr.open_datatree(zarr_path, engine=\"zarr\", mask_and_scale=False, chunks={})\n",
    "        data = datatree.measurements.reflectance[self.res_key]\n",
    "\n",
    "        # Get chunk layout\n",
    "        band  = self.bands[0]\n",
    "        chunk_size_y = data[band].chunksizes[self.y_res][0]\n",
    "        chunk_size_x = data[band].chunksizes[self.x_res][0]\n",
    "        nb_chunks_y = len(data[band].chunksizes[self.y_res])\n",
    "        nb_chunks_x = len(data[band].chunksizes[self.x_res])\n",
    "\n",
    "        all_chunks, all_masks = [], []\n",
    "\n",
    "        for row in range(nb_chunks_y):\n",
    "            for col in range(nb_chunks_x):\n",
    "                y_start = row * chunk_size_y\n",
    "                x_start = col * chunk_size_x\n",
    "                chunk_ds = data.isel(\n",
    "                    {self.y_res: slice(y_start, y_start + chunk_size_y),\n",
    "                     self.x_res: slice(x_start, x_start + chunk_size_x)}\n",
    "                )\n",
    "\n",
    "                chunk_array = chunk_ds.to_dataset().to_dataarray().values\n",
    "                chunk_array, mask_array = normalize(chunk_array)\n",
    "\n",
    "                # Convert to torch [C, H, W]\n",
    "                chunk_tensor = torch.from_numpy(chunk_array).float()\n",
    "                mask_tensor = torch.from_numpy(mask_array).float()\n",
    "\n",
    "                # Resize to target size\n",
    "                chunk_tensor = F.interpolate(\n",
    "                    chunk_tensor.unsqueeze(0),  # add batch dim\n",
    "                    size=self.target_size,\n",
    "                    mode='bilinear',\n",
    "                    align_corners=False\n",
    "                ).squeeze(0)\n",
    "\n",
    "                mask_tensor = F.interpolate(\n",
    "                    mask_tensor.unsqueeze(0),\n",
    "                    size=self.target_size,\n",
    "                    mode='nearest'\n",
    "                ).squeeze(0)\n",
    "\n",
    "                all_chunks.append(chunk_tensor)\n",
    "                all_masks.append(mask_tensor)\n",
    "\n",
    "        chunks_grid = torch.stack(all_chunks).view(nb_chunks_y, nb_chunks_x, *all_chunks[0].shape)\n",
    "        masks_grid = torch.stack(all_masks).view(nb_chunks_y, nb_chunks_x, *all_masks[0].shape)\n",
    "        meta = (nb_chunks_y, nb_chunks_x, chunk_size_y, chunk_size_x)\n",
    "        return chunks_grid, masks_grid, meta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9228d247",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "res = \"60m\"\n",
    "bands = ['b01','b02','b03','b04','b05','b06','b07','b09','b11','b12','b8a']\n",
    "batch_size = 2\n",
    "\n",
    "dataset = Sentinel2ZarrDataset(df_x=df_test_output, res=res, bands=bands)\n",
    "loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "with tqdm(total=len(loader.dataset), ncols=100, colour='#3eedc4') as t:\n",
    "    t.set_description(\"Processing\")\n",
    "    for chunks_grid, masks_grid, meta in loader:\n",
    "        B, ny, nx, C, H, W = chunks_grid.shape\n",
    "        chunks_tensor = chunks_grid.view(B*ny*nx, C, H, W).to(device)\n",
    "        masks_tensor = masks_grid.view(B*ny*nx, C, H, W).to(device)\n",
    "        t.update(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354af9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a01d0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- Parameters ----------------\n",
    "res = \"60m\"\n",
    "bands = ['b01', 'b02', 'b03', 'b04', 'b05', 'b06', 'b07', 'b09', 'b11', 'b12', 'b8a']\n",
    "df_test_input, df_test_output = prepare_paths(TEST_DIR)\n",
    "batch_size = len(df_test_output)\n",
    "\n",
    "train_dataset = Sentinel2ZarrDataset(df_x=df_test_output, res=res, bands=bands,target_size=(320,320) )\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0333411a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- Loop over dataset ----------------\n",
    "with tqdm(total=len(train_loader.dataset), ncols=100, colour='#3eedc4') as t:\n",
    "    t.set_description(\"Processing\")\n",
    "    for batch_idx, (chunks_grid, masks_grid, meta) in enumerate(train_loader):\n",
    "        B, ny, nx, C, H, W = chunks_grid.shape\n",
    "        chunks_tensor = chunks_grid.view(B*ny*nx, C, H, W).to(device)\n",
    "        masks_tensor = masks_grid.view(B*ny*nx, C, H, W).to(device)\n",
    "\n",
    "        for batch_scene in range(B):\n",
    "            scene_index = batch_idx * batch_size + batch_scene\n",
    "            scene_chunks = chunks_grid[batch_scene]  # [ny, nx, C, H, W]\n",
    "\n",
    "            row_idx, col_idx = 0, 4\n",
    "            rebuilt_chunk = scene_chunks[row_idx, col_idx]  # [C, H, W]\n",
    "            # Load same chunk from Zarr\n",
    "            x_path = df_test_output[\"path\"].iloc[scene_index] + \".zarr\"\n",
    "            dt = xr.open_datatree(x_path, engine=\"zarr\", mask_and_scale=False, chunks={})\n",
    "            data_tree = dt.measurements.reflectance[f\"r{res}\"]\n",
    "\n",
    "            y_start, x_start = row_idx * H, col_idx * W\n",
    "            original_chunk = data_tree.isel(\n",
    "                {f\"y_{res}\": slice(y_start, y_start + H),\n",
    "                 f\"x_{res}\": slice(x_start, x_start + W)}\n",
    "            ).to_dataset().to_dataarray()\n",
    "\n",
    "            # ---------------- Plot ----------------\n",
    "            plt.figure(figsize=(8,6))\n",
    "            plt.imshow(rebuilt_chunk[0].cpu().numpy(), cmap=\"viridis\")\n",
    "            plt.title(f\"Rebuilt - Scene {scene_index} - Chunk ({row_idx},{col_idx}) - Band 0\")\n",
    "            plt.colorbar()\n",
    "            plt.show()\n",
    "\n",
    "            plt.figure(figsize=(8,6))\n",
    "            plt.imshow(original_chunk[0].values, cmap=\"viridis\")\n",
    "            plt.title(f\"Original - Scene {scene_index} - Chunk ({row_idx},{col_idx}) - Band 0\")\n",
    "            plt.colorbar()\n",
    "            plt.show()\n",
    "\n",
    "        t.update(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e9243e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## checkpoint test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc37086",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/home/ubuntu/project/sentinel-2-ai-compressor/src/results/2025-08-14_14-03-11/checkpoints/best_model.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc07e57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = \"60m\"\n",
    "bands = ['b01', 'b02', 'b03', 'b04', 'b05', 'b06', 'b07', 'b09', 'b11', 'b12', 'b8a']\n",
    "batch_size = 2\n",
    "\n",
    "test_dataset = Sentinel2ZarrDataset(df_x=df_test_output, res=res, bands=bands)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621a18dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "from zarr import config\n",
    "from model_zoo.models import define_model\n",
    "from utils.utils import load_config\n",
    "\n",
    "config = load_config(\"/home/ubuntu/project/sentinel-2-ai-compressor/src/results/2025-08-14_15-03-31/config.yaml\")\n",
    "\n",
    "\n",
    "model = define_model(\n",
    "        name=config['MODEL']['model_name'],\n",
    "        encoder_name=config['MODEL']['encoder_name'],\n",
    "        encoder_weights=config['MODEL']['encoder_weights'],\n",
    "        in_channel=len(config['DATASET']['bands']),\n",
    "        out_channels=len(config['DATASET']['bands']),\n",
    "        activation=config['MODEL']['activation']\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822c5ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "checkpoint_path = \"/home/ubuntu/project/sentinel-2-ai-compressor/src/results/2025-08-14_15-03-31/checkpoints\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004ddd66",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(os.path.join(checkpoint_path, \"best_model.pth\")))\n",
    "model = model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4bb66be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- Loop over dataset ----------------\n",
    "with tqdm(total=len(test_loader.dataset), ncols=100, colour='#3eedc4') as t:\n",
    "    t.set_description(\"Processing\")\n",
    "    for batch_idx, (chunks_grid, masks_grid, meta) in enumerate(test_loader):\n",
    "        B, ny, nx, C, H, W = chunks_grid.shape\n",
    "        chunks_tensor = chunks_grid.view(B * ny * nx, C, H, W).to(device)\n",
    "        masks_tensor = masks_grid.view(B * ny * nx, C, H, W).to(device)\n",
    "        outputs = model(chunks_tensor)\n",
    "        print(masks_tensor.shape)\n",
    "        print(outputs.shape)\n",
    "        t.update(B)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3261fdf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(chunks_tensor[13,10,:,:].detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce17d364",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(outputs[13,10,:,:].detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a105da9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_processor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
